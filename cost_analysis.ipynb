{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fine_tuning_parameters.json', 'r') as f:\n",
    "    fine_tuning_parameters_json = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "transformer_family = fine_tuning_parameters_json['transformer_family']\n",
    "tokenizer_family = fine_tuning_parameters_json['tokenizer_family']\n",
    "optimizer_family = fine_tuning_parameters_json['optimizer_family']\n",
    "scheduler_family = fine_tuning_parameters_json['scheduler_family']\n",
    "\n",
    "if transformer_family == 'AutoModelForCausalLM':\n",
    "    from transformers import AutoModelForCausalLM as Model_Family\n",
    "elif transformer_family == 'AutoModelForSequenceClassification':\n",
    "    from transformers import AutoModelForSequenceClassification as Model_Family\n",
    "elif transformer_family == 'AutoModelForTokenClassification':\n",
    "    from transformers import AutoModelForTokenClassification as Model_Family\n",
    "#TODO: Add more families, error handling and more efficient structure\n",
    "\n",
    "if tokenizer_family == 'AutoTokenizer':\n",
    "    from transformers import AutoTokenizer as Tokenizer_Family\n",
    "elif tokenizer_family == 'AutoTokenizerFast':\n",
    "    from transformers import AutoTokenizerFast as Tokenizer_Family\n",
    "\n",
    "if optimizer_family == 'AdamW':\n",
    "    from transformers import AdamW as Optimizer_Family\n",
    "elif optimizer_family == 'Adam':\n",
    "    from transformers import Adam as Optimizer_Family\n",
    "elif optimizer_family == 'SGD':\n",
    "    from transformers import SGD as Optimizer_Family\n",
    "\n",
    "if scheduler_family == 'get_linear_schedule_with_warmup':\n",
    "    from transformers import get_linear_schedule_with_warmup as Scheduler_Family\n",
    "elif scheduler_family == 'get_constant_schedule_with_warmup':\n",
    "    from transformers import get_constant_schedule_with_warmup as Scheduler_Family\n",
    "elif scheduler_family == 'get_cosine_schedule_with_warmup':\n",
    "    from transformers import get_cosine_schedule_with_warmup as Scheduler_Family\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "def train(dataloader, optimizer, device='cuda'):\n",
    "    model.train() \n",
    "    for batch in tqdm(dataloader, total=len(dataloader), desc='Training...'):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs[0].item()\n",
    "        losses.append(loss)\n",
    "    return np.mean(losses)\n",
    "\n",
    "def process_dataframe(df, languages, max_seq_len, tokenizer, num_repos):\n",
    "    ds = []\n",
    "    for i in tqdm(range(num_repos), desc='Processing dataframe Repositories'):\n",
    "        for language in languages:\n",
    "            code_dict = eval(df[language][i])\n",
    "            files_names = list(code_dict.keys())\n",
    "            for file_name in files_names:\n",
    "                code = code_dict[file_name]\n",
    "                inputs = tokenizer(code, return_tensors='pt', max_length=max_seq_len, truncation=True, padding='max_length')\n",
    "                input_ids = inputs.input_ids.squeeze()\n",
    "                labels = input_ids.clone()\n",
    "                labels[0] = -100\n",
    "                ds.append({'input_ids': input_ids, 'labels': labels, \n",
    "                              'language': language, 'file_name': file_name, 'code': code})\n",
    "    return ds\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Setup\n",
    "transformer_args = fine_tuning_parameters_json['transformer_args']\n",
    "tokenizer_args = fine_tuning_parameters_json['tokenizer_args']\n",
    "\n",
    "#TODO: IF there are more named arguments, add them to the dictionary and pass them to the model and tokenizer\n",
    "\n",
    "tokenizer_name = tokenizer_args['tokenizer_name']\n",
    "model_name = transformer_args['model_name']\n",
    "\n",
    "tokenizer = Tokenizer_Family.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token else tokenizer.eos_token\n",
    "\n",
    "model = Model_Family.from_pretrained(model_name)    \n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Data\n",
    "data_path = fine_tuning_parameters_json['data_path']\n",
    "languages = fine_tuning_parameters_json['languages']\n",
    "max_seq_len = fine_tuning_parameters_json['max_seq_len']\n",
    "repo_count = fine_tuning_parameters_json['repo_count']\n",
    "val_size = fine_tuning_parameters_json['val_size']\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "ds = process_dataframe(data, languages, max_seq_len, tokenizer, repo_count)\n",
    "val_size = int(val_size * len(ds))\n",
    "train_size = len(ds) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "\n",
    "# =============================================================================\n",
    "# Hyperparameters\n",
    "batch_size = fine_tuning_parameters_json['batch_size']\n",
    "lr = fine_tuning_parameters_json['optimizer_args']['lr']\n",
    "epochs = fine_tuning_parameters_json['epochs']\n",
    "warmup = fine_tuning_parameters_json['scheduler_args']['num_warmup_steps']\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer\n",
    "#TODO: Use the optimizer_args dictionary to pass the arguments to the optimizer\n",
    "optimizer = Optimizer_Family(model.parameters(), lr=lr)\n",
    "\n",
    "# Scheduler\n",
    "#TODO: Use the scheduler_args dictionary to pass the arguments to the scheduler\n",
    "scheduler = Scheduler_Family(optimizer, num_warmup_steps=warmup, num_training_steps=len(train_dataloader)*epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# Training\n",
    "for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "    train(train_dataloader, optimizer)\n",
    "    val_loss = evaluate(val_dataloader)\n",
    "    print(f'Epoch: {epoch}, Val Loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
