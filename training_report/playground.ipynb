{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "\n",
    "def calc_avg_line_length(text):\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    return sum(line_lengths) / len(line_lengths)\n",
    "\n",
    "def calc_max_line_length(text):\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    return max(line_lengths)\n",
    "\n",
    "def calc_alphanum_fraction(text):\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    alphanum = sum(c.isalnum() for c in text)\n",
    "    return alphanum / len(text)\n",
    "\n",
    "\n",
    "def dataset_from_df(df):\n",
    "    dataset = {\n",
    "        'repo_name': [],\n",
    "        'text': [],\n",
    "        'avg_line_length': [],\n",
    "        'max_line_length': [],\n",
    "        'alphnanum_fraction': [],\n",
    "    }\n",
    "    for i in tqdm(range(len(df))):\n",
    "        repo = df.iloc[i]\n",
    "        text = repo['text']\n",
    "        dataset['repo_name'].append(repo['repo_name'])\n",
    "        dataset['text'].append(text)\n",
    "        dataset['avg_line_length'].append(calc_avg_line_length(text))\n",
    "        dataset['max_line_length'].append(calc_max_line_length(text))\n",
    "        dataset['alphnanum_fraction'].append(calc_alphanum_fraction(text))\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(hf_dataset):\n",
    "    train_test_ds = hf_dataset['train'].train_test_split(test_size=0.3)\n",
    "    test_valid = train_test_ds['test'].train_test_split(test_size=0.3)\n",
    "    train_test_valid_dataset = DatasetDict({\n",
    "        'train': train_test_ds['train'],\n",
    "        'test': test_valid['test'],\n",
    "        'valid': test_valid['train']})\n",
    "    return train_test_valid_dataset\n",
    "\n",
    "def huggingface_dataset_from_df(df):\n",
    "    dataset = dataset_from_df(df)\n",
    "    with open('hf_ds.pkl', 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    hf_dataset = load_dataset(\"pandas\", data_files='hf_ds.pkl')\n",
    "    os.remove('hf_ds.pkl')\n",
    "    hf_dataset = split_dataset(hf_dataset)\n",
    "    return hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = './security.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "df.rename(columns={'code':'text'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = huggingface_dataset_from_df(df)\n",
    "hf_token = os.environ['HUGGINGFACE_TOKEN']\n",
    "login(token=hf_token)\n",
    "hf_dataset.push_to_hub(\"Python-Security-Code-Dataset\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = './react2.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "df.rename(columns={'JS_files':'text'}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_react = pd.read_csv('./react2.csv')\n",
    "df_react.rename(columns={'text':'code'}, inplace=True)\n",
    "df_security = pd.read_csv('./security.csv')\n",
    "df_react['query'] = 'react 18.0'\n",
    "\n",
    "#fill nan with the most common value for each column in each dataframe\n",
    "df_react = df_react.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "df_security = df_security.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "\n",
    "# drop nan from both dataframes and shuffle then reset index\n",
    "df_react.dropna(inplace=True)\n",
    "df_security.dropna(inplace=True)\n",
    "df_react = df_react.sample(frac=1).reset_index(drop=True)\n",
    "df_security = df_security.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df_react.shape)\n",
    "print(df_security.shape)\n",
    "\n",
    "#put column in the same order [repo_name, file_path, code, query]\n",
    "df_react = df_react[['repo_name', 'file_path', 'code', 'query']]\n",
    "df_security = df_security[['repo_name', 'file_path', 'code', 'query']]\n",
    "\n",
    "\n",
    "#Save the dataframes back to csv as processed data\n",
    "df_react.to_csv('./react_processed.csv', index=False)\n",
    "df_security.to_csv('./security_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_react.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_security.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = huggingface_dataset_from_df(df)\n",
    "hf_token = os.environ['HUGGINGFACE_TOKEN']\n",
    "login(token=hf_token)\n",
    "hf_dataset.push_to_hub(\"Python-React-Code-Dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = 'loss.csv'\n",
    "loss_df = pd.read_csv(filename)\n",
    "cols = loss_df.columns\n",
    "for col in cols:\n",
    "    if col.endswith('loss'):\n",
    "        loss_df.rename(columns={col:'loss'}, inplace=True)\n",
    "    elif col.endswith('Step'):\n",
    "        continue\n",
    "    else:\n",
    "        loss_df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_df['loss'])\n",
    "\n",
    "ema = loss_df['loss'].ewm(span=20).mean()\n",
    "plt.plot(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge LoRa Model with Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setp 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_token =''\n",
    "os.environ['HUGGINGFACE_TOKEN'] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.environ['HUGGINGFACE_TOKEN'])\n",
    "LORA_MODEL_ID = \"MuhammedSaeed/LLMJS\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load the LoRa Model and the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(LORA_MODEL_ID)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,trust_remote_code=True)\n",
    "lora_model = PeftModel.from_pretrained(base_model, LORA_MODEL_ID)\n",
    "print(lora_model)\n",
    "print(base_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Merge and Unload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = lora_model.merge_and_unload()\n",
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Push the model to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_MODEL_ID = \"ammarnasr/codegen2-1B-react\"\n",
    "merged_model.push_to_hub(MERGED_MODEL_ID, use_auth_token=True)\n",
    "tokenizer.push_to_hub(MERGED_MODEL_ID, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finalizing the model for deployment\n",
    "- Copy the config.json file from the base model to the merged model\n",
    "- Copy the configuration*.py file from the base model to the merged model\n",
    "- Copy the modelling*.py file from the base model to the merged model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inferencing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39202a672e648488fa2a28b52ffbce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/166k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edin\\anaconda3\\envs\\amazon\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Edin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad07a24eea14d1881737d28b52a0f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aee1e367dd142988fae4812044f8cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd552976b9e341af9407271dd43d24f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c6f495b9ac4b7c888710431cf41394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68dd769ab624b4092592505fb7f2aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c927da5a8d94d1ca6970234d64e6437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bc2ebb883c458682655df1ee49a057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eee960e5424a439af60a5e59b27d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_id = \"ammarnasr/codegen2-1B-react\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"def hello_world():\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "generated_ids = model.generate(input_ids, max_length=128)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
