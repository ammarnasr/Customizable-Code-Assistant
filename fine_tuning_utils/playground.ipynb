{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib \n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "def calc_avg_line_length(text):\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    return sum(line_lengths) / len(line_lengths)\n",
    "\n",
    "def calc_max_line_length(text):\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    return max(line_lengths)\n",
    "\n",
    "def calc_alphanum_fraction(text):\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    alphanum = sum(c.isalnum() for c in text)\n",
    "    return alphanum / len(text)\n",
    "\n",
    "def dataset_from_df(df):\n",
    "    dataset = {\n",
    "        'repo_name': [],\n",
    "        'repo_url': [],\n",
    "        'repo_description': [],\n",
    "        'repo_stars': [],\n",
    "        'repo_forks': [],\n",
    "        'repo_last_updated': [],\n",
    "        'repo_created_at': [],\n",
    "        'repo_size': [],\n",
    "        'repo_license': [],\n",
    "        'language': [],\n",
    "        'text': [],\n",
    "        'avg_line_length': [],\n",
    "        'max_line_length': [],\n",
    "        'alphnanum_fraction': [],\n",
    "    }\n",
    "    for i in tqdm(range(len(df))):\n",
    "        repo = df.iloc[i]\n",
    "        code = repo['code']\n",
    "        for programming_language in code:\n",
    "            code_files = code[programming_language]\n",
    "            for code_file in code_files:\n",
    "                text = code_files[code_file]\n",
    "                dataset['repo_name'].append(repo['name'])\n",
    "                dataset['repo_url'].append(repo['url'])\n",
    "                dataset['repo_description'].append(repo['description'])\n",
    "                dataset['repo_stars'].append(repo['stars'])\n",
    "                dataset['repo_forks'].append(repo['forks'])\n",
    "                dataset['repo_last_updated'].append(repo['last_updated'])\n",
    "                dataset['repo_created_at'].append(repo['created'])\n",
    "                dataset['repo_size'].append(repo['size'])\n",
    "                dataset['repo_license'].append(repo['license'])\n",
    "                dataset['language'].append(programming_language)\n",
    "                dataset['text'].append(text)\n",
    "                dataset['avg_line_length'].append(calc_avg_line_length(text))\n",
    "                dataset['max_line_length'].append(calc_max_line_length(text))\n",
    "                dataset['alphnanum_fraction'].append(calc_alphanum_fraction(text))\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    return dataset\n",
    "\n",
    "def huggingface_dataset_from_df(df):\n",
    "    dataset = dataset_from_df(df)\n",
    "    with open('hf_ds.pkl', 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    hf_dataset = load_dataset(\"pandas\", data_files='hf_ds.pkl')\n",
    "    os.remove('hf_ds.pkl')\n",
    "    return hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblibs_path = '.././saved_searches'\n",
    "joblibs = os.listdir(joblibs_path)\n",
    "joblibs = [f for f in joblibs if f.endswith('.joblib')]\n",
    "joblibs = [os.path.join(joblibs_path, f) for f in joblibs]\n",
    "df = joblib.load(joblibs[0])\n",
    "raw_datasets = huggingface_dataset_from_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========================================================== ========================================================== ========================================================== =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Edin/.cache/huggingface/datasets/ammarnasr___parquet/ammarnasr--Customizable-Code-Assistant-Data-3467676a1c1517c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abce5b5f5abc4d48ad06461ba0f2b1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Edin\\.cache\\huggingface\\datasets\\ammarnasr___parquet\\ammarnasr--Customizable-Code-Assistant-Data-3467676a1c1517c6\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-f6cd86854f64fec7.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64dbd68a18824b98ace0425be09d1d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f370686d34a34359b70f4daec73b5120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1c2a920b2240c09b5fd1759e1676bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: 2713 training samples\n",
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "from  datasets  import  load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Downloading and loading a dataset from the hub.\n",
    "raw_datasets = load_dataset(\"ammarnasr/Customizable-Code-Assistant-Data\")\n",
    "\n",
    "# Filtering the dataset to only Python examples.\n",
    "raw_datasets = raw_datasets.filter(lambda example: example['language'] == 'Python')\n",
    "\n",
    "# Splitting the dataset into train, test, and validation sets.\n",
    "train_testvalid = raw_datasets['train'].train_test_split(0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(0.5)\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "context_length = 128\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "print(f\"Tokenized dataset: {tokenized_datasets['train'].num_rows} training samples\")\n",
    "      \n",
    "# Preparing the model\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# Preparing the data collator\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Preparing the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"gpt2-from-scratch-customizable-code-assistant\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 device(s).\n",
      "Device: 0\n",
      "  Name: NVIDIA GeForce GTX 1650\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 14\n",
      "  CUDA Cores: 896\n",
      "  CUDA Architecture: Turing\n",
      "  FP64 ops per cycle: 2\n",
      "  FP32 ops per cycle: 64\n",
      "  FP16 ops per cycle: 128\n",
      "  INT8 ops per cycle: 256\n",
      "  Concurrent threads: 14336\n",
      "  GPU clock: 1515 MHz\n",
      "  Memory clock: 6001 MHz\n",
      "  Total Memory: 4095 MiB\n",
      "  Free Memory: 2733 MiB\n",
      "{\n",
      "    \"device_0\": {\n",
      "        \"Name\": \"NVIDIA GeForce GTX 1650\",\n",
      "        \"Compute Capability\": \"7.5\",\n",
      "        \"Multiprocessors\": 14,\n",
      "        \"CUDA Cores\": 896,\n",
      "        \"CUDA Architecture\": \"Turing\",\n",
      "        \"Ops per cycle\": {\n",
      "            \"FP64\": 2,\n",
      "            \"FP32\": 64,\n",
      "            \"FP16\": 128,\n",
      "            \"INT8\": 256\n",
      "        },\n",
      "        \"Concurrent threads\": 14336,\n",
      "        \"GPU clock\": 1515.0,\n",
      "        \"Memory clock\": 6001.0,\n",
      "        \"Total Memory (MiB)\": 4095.6875,\n",
      "        \"Free Memory (MiB)\": 2733.7000007629395\n",
      "    }\n",
      "}\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "GPU FLOPS: 2.71 TFLOPS\n",
      "Model Parameters: 124.24M\n",
      "Dataset Size: 347264 tokens\n",
      "Training Time: 26.49 hours\n",
      "Optimal Dataset Size: 2484.85M tokens (7155.50x current dataset size)\n"
     ]
    }
   ],
   "source": [
    "# Transformer Math : \n",
    "# basic math related to computation and memory usage for transformers\n",
    "\n",
    "# 1. Information About Local Accelerator (GPU/CPU)\n",
    "# This includes all the available devices and each one's :\n",
    "# Name, Compute Capability, Multiprocessors, CUDA Cores, Concurrent threads, GPU clock, Memory clock, Total Memory and Free Memory.\n",
    "from cuda_utils import  main, gpu_flops\n",
    "import json\n",
    "cuda_info = main()\n",
    "print(json.dumps(cuda_info,  indent=4))\n",
    "\n",
    "# 2. Compute Requirements\n",
    "# The basic equation giving the cost to train a transformer model is given by:\n",
    "#                                    C = tao*T = 6*P*D\n",
    "# where:\n",
    "# C is the compute required to train the transformer model, in total floating point operations (FLOPs)\n",
    "# C = C_forward + C_backward\n",
    "# C_forward  =  2*P*D\n",
    "# C_backward =  4*P*D\n",
    "# tao is the aggregate throughput of your hardware setup (tao = (No. of GPUs) * (Actual FLOPs/GPU), in FLOPs\n",
    "# T is the time spent training the model, in seconds\n",
    "# P is the number of parameters in the transformer model\n",
    "# D is the dataset size, in tokens\n",
    "\n",
    "#3. Estimating GPU Actual FLOPs (tao)\n",
    "# Estimating GPU FLOPs accurately can be challenging due to the complexity of GPU architectures and optimizations. However, here is a simple formula to estimate a GPU FLOP/s:\n",
    "# Total GPU FLOPS/s = GPU clock * cores * flops_per_clock_cycle * fp_precision\n",
    "for gpu_no in cuda_info:\n",
    "    gpu = cuda_info[gpu_no]\n",
    "    print(f\"GPU: {gpu['Name']}\")\n",
    "    print(f\"GPU FLOPS: {gpu_flops(gpu):.2f} TFLOPS\")\n",
    "    tao = gpu_flops(gpu) # TFLOPS\n",
    "\n",
    "#4. Estimating Model Parameters (P)\n",
    "P = model.num_parameters()\n",
    "print(f\"Model Parameters: {P/1000**2:.2f}M\")\n",
    "\n",
    "#5. Estimating Dataset Size (D)\n",
    "D = 0\n",
    "for input_ids in tokenized_datasets[\"train\"][\"input_ids\"]:\n",
    "    D += len(input_ids)\n",
    "print(f\"Dataset Size: {D} tokens\")\n",
    "\n",
    "#6. Estimating Training Time (T)\n",
    "# Calculate training time based on the formula tao*T = 6*P*D. Note that ao needs to be in FLOPS/s not TFLOPS/s, to convert TFLOPS/s to FLOPS/s multiply by 1000**3.\n",
    "T = 6*P*D/(tao*1000**3)\n",
    "print(f\"Training Time: {T/3600:.2f} hours\")\n",
    "\n",
    "\n",
    "#7. Parameter vs Dataset Tradeoffs\n",
    "# Although strictly speaking you can train a transformer for as many tokens as you like, the number of tokens trained can highly impact both the computing costs and the final model performance making striking the right balance important.\n",
    "# compute optimal language model has a number of parameters and a dataset size that satisfies the approximation D = 20*P.\n",
    "optimal_D = 20*P\n",
    "print(f\"Optimal Dataset Size: {optimal_D / 1000**2:.2f}M tokens ({optimal_D / D:.2f}x current dataset size)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['train']['input_ids'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347264"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 * len(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [973, 6661, 14, 8528, 14, 1353, 978, 549, 63, 875, 63, 961, 173, 973, 6661, 14, 5588, 978, 3744, 173, 973, 6661, 14, 1824, 978, 7252, 13076, 173, 173, 973, 2956, 63, 15766, 978, 1519, 173, 973, 2956, 63, 15766, 14, 1824, 978, 2002, 3002, 4391, 173, 20245, 63, 17370, 63, 5362, 3686, 63, 3220, 233, 3744, 439, 3997, 26, 22450, 13, 375, 528, 173, 20245, 63, 17370, 63, 5362, 3686, 63, 3220, 233, 3744, 8, 232, 333, 3997, 26, 22450, 13, 5500, 485, 922, 10426, 63, 293, 7095, 4063, 63, 22450, 7568, 173, 9, 4391, 173, 692, 16544, 7755, 6252, 83, 5791, 22265, 8, 40674, 274, 312, 509, 562, 6134, 8, 248, 274, 222, 272, 14, 1318, 233, 2002, 3002, 323, 312, 509, 1737, 63, 4092, 63]}\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_datasets['train']:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5797a0b23041709fb0363c7ee68687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "for i in tqdm(tokenized_datasets['train'], total=len(tokenized_datasets['train'])):\n",
    "    x= len(i['input_ids'])\n",
    "    if x != 128:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
