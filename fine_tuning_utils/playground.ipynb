{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datasets  import  load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Downloading and loading a dataset from the hub.\n",
    "raw_datasets = load_dataset(\"ammarnasr/Customizable-Code-Assistant-Data\")\n",
    "\n",
    "# Filtering the dataset to only Python examples.\n",
    "raw_datasets = raw_datasets.filter(lambda example: example['language'] == 'Python')\n",
    "\n",
    "# Splitting the dataset into train, test, and validation sets.\n",
    "train_testvalid = raw_datasets['train'].train_test_split(0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(0.5)\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "context_length = 128\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "print(f\"Tokenized dataset: {tokenized_datasets['train'].num_rows} training samples\")\n",
    "      \n",
    "# Preparing the model\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# Preparing the data collator\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Preparing the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"gpt2-from-scratch-customizable-code-assistant\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Math : \n",
    "# basic math related to computation and memory usage for transformers\n",
    "\n",
    "# 1. Information About Local Accelerator (GPU/CPU)\n",
    "# This includes all the available devices and each one's :\n",
    "# Name, Compute Capability, Multiprocessors, CUDA Cores, Concurrent threads, GPU clock, Memory clock, Total Memory and Free Memory.\n",
    "from cuda_utils import  main, gpu_flops\n",
    "import json\n",
    "cuda_info = main()\n",
    "print(json.dumps(cuda_info,  indent=4))\n",
    "\n",
    "# 2. Compute Requirements\n",
    "# The basic equation giving the cost to train a transformer model is given by:\n",
    "#                                    C = tao*T = 6*P*D\n",
    "# where:\n",
    "# C is the compute required to train the transformer model, in total floating point operations (FLOPs)\n",
    "# C = C_forward + C_backward\n",
    "# C_forward  =  2*P*D\n",
    "# C_backward =  4*P*D\n",
    "# tao is the aggregate throughput of your hardware setup (tao = (No. of GPUs) * (Actual FLOPs/GPU), in FLOPs\n",
    "# T is the time spent training the model, in seconds\n",
    "# P is the number of parameters in the transformer model\n",
    "# D is the dataset size, in tokens\n",
    "\n",
    "#3. Estimating GPU Actual FLOPs (tao)\n",
    "# Estimating GPU FLOPs accurately can be challenging due to the complexity of GPU architectures and optimizations. However, here is a simple formula to estimate a GPU FLOP/s:\n",
    "# Total GPU FLOPS/s = GPU clock * cores * flops_per_clock_cycle * fp_precision\n",
    "for gpu_no in cuda_info:\n",
    "    gpu = cuda_info[gpu_no]\n",
    "    print(f\"GPU: {gpu['Name']}\")\n",
    "    print(f\"GPU FLOPS: {gpu_flops(gpu):.2f} TFLOPS\")\n",
    "    tao = gpu_flops(gpu) # TFLOPS\n",
    "\n",
    "#4. Estimating Model Parameters (P)\n",
    "P = model.num_parameters()\n",
    "print(f\"Model Parameters: {P/1000**2:.2f}M\")\n",
    "\n",
    "#5. Estimating Dataset Size (D)\n",
    "D = 0\n",
    "for input_ids in tokenized_datasets[\"train\"][\"input_ids\"]:\n",
    "    D += len(input_ids)\n",
    "print(f\"Dataset Size: {D} tokens\")\n",
    "\n",
    "#6. Estimating Training Time (T)\n",
    "# Calculate training time based on the formula tao*T = 6*P*D. Note that ao needs to be in FLOPS/s not TFLOPS/s, to convert TFLOPS/s to FLOPS/s multiply by 1000**3.\n",
    "T = 6*P*D/(tao*1000**3)\n",
    "print(f\"Training Time: {T/3600:.2f} hours\")\n",
    "\n",
    "\n",
    "#7. Parameter vs Dataset Tradeoffs\n",
    "# Although strictly speaking you can train a transformer for as many tokens as you like, the number of tokens trained can highly impact both the computing costs and the final model performance making striking the right balance important.\n",
    "# compute optimal language model has a number of parameters and a dataset size that satisfies the approximation D = 20*P.\n",
    "optimal_D = 20*P\n",
    "print(f\"Optimal Dataset Size: {optimal_D / 1000**2:.2f}M tokens ({optimal_D / D:.2f}x current dataset size)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda_utils import  main, list_common_gpus, custome_gpu_info\n",
    "import json\n",
    "\n",
    "available_gpus = list_common_gpus()\n",
    "print(f\"Available GPUs: {available_gpus}\")\n",
    "\n",
    "\n",
    "t4_cuda_info = custome_gpu_info('NVIDIA T4')\n",
    "cuda_info = main(verbose=False)\n",
    "print(json.dumps(t4_cuda_info,  indent=4))\n",
    "print('------------------')\n",
    "print(json.dumps(cuda_info,  indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('llms_info.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Model</th>\n",
       "      <th>Arch.</th>\n",
       "      <th>Size</th>\n",
       "      <th>Vocab</th>\n",
       "      <th>Context</th>\n",
       "      <th>Init. from</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Training</th>\n",
       "      <th>PL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12</td>\n",
       "      <td>CuBERT</td>\n",
       "      <td>BERT</td>\n",
       "      <td>350M</td>\n",
       "      <td>50K</td>\n",
       "      <td>1024</td>\n",
       "      <td>-</td>\n",
       "      <td>9.3B</td>\n",
       "      <td>93B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02</td>\n",
       "      <td>CodeBERT</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>125M</td>\n",
       "      <td>50K</td>\n",
       "      <td>512</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>20GB</td>\n",
       "      <td>105B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09</td>\n",
       "      <td>GraphCode-BERT</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>125M</td>\n",
       "      <td>50K</td>\n",
       "      <td>640</td>\n",
       "      <td>CodeBERT</td>\n",
       "      <td>20GB</td>\n",
       "      <td>131B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08</td>\n",
       "      <td>SynCoBERT</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>125M</td>\n",
       "      <td>50K</td>\n",
       "      <td>512</td>\n",
       "      <td>CodeBERT</td>\n",
       "      <td>20GB</td>\n",
       "      <td>7B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10</td>\n",
       "      <td>DISCO</td>\n",
       "      <td>BERT</td>\n",
       "      <td>100M</td>\n",
       "      <td>20K</td>\n",
       "      <td>512</td>\n",
       "      <td>-</td>\n",
       "      <td>1.8GB</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05</td>\n",
       "      <td>Code-MVP</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>125M</td>\n",
       "      <td>50K</td>\n",
       "      <td>512</td>\n",
       "      <td>GraphCodeBERT</td>\n",
       "      <td>2GB</td>\n",
       "      <td>39B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-05</td>\n",
       "      <td>GPT-C</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>374M</td>\n",
       "      <td>60K</td>\n",
       "      <td>1024</td>\n",
       "      <td>-</td>\n",
       "      <td>11B</td>\n",
       "      <td>270B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02</td>\n",
       "      <td>CodeGPT</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>124M</td>\n",
       "      <td>50K</td>\n",
       "      <td>1024</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>2GB</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-02</td>\n",
       "      <td>PolyCoder</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>160M-2.7B</td>\n",
       "      <td>50K</td>\n",
       "      <td>2048</td>\n",
       "      <td>-</td>\n",
       "      <td>254GB</td>\n",
       "      <td>39B</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-03</td>\n",
       "      <td>CodeGen-Multi(Mono)</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>350M-16.1B</td>\n",
       "      <td>50K</td>\n",
       "      <td>2048</td>\n",
       "      <td>-</td>\n",
       "      <td>1.6TB(1.8TB)/506B(577B)</td>\n",
       "      <td>1T(1.2T)</td>\n",
       "      <td>6(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-04</td>\n",
       "      <td>InCoder</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>6.7B</td>\n",
       "      <td>50K</td>\n",
       "      <td>2048</td>\n",
       "      <td>-</td>\n",
       "      <td>204GB</td>\n",
       "      <td>52B</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-06</td>\n",
       "      <td>PyCodeGPT</td>\n",
       "      <td>GPT-Neo</td>\n",
       "      <td>110M</td>\n",
       "      <td>32K</td>\n",
       "      <td>1024</td>\n",
       "      <td>-</td>\n",
       "      <td>96GB</td>\n",
       "      <td>100B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-07</td>\n",
       "      <td>PanGu-Coder</td>\n",
       "      <td>PanGu-α</td>\n",
       "      <td>317M-2.6B</td>\n",
       "      <td>42K</td>\n",
       "      <td>1024</td>\n",
       "      <td>-</td>\n",
       "      <td>147GB</td>\n",
       "      <td>230B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>SantaCoder</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>1.1B</td>\n",
       "      <td>49K</td>\n",
       "      <td>2048</td>\n",
       "      <td>-</td>\n",
       "      <td>268GB</td>\n",
       "      <td>236B</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>CodeGeeX</td>\n",
       "      <td>PanGu-α</td>\n",
       "      <td>13B</td>\n",
       "      <td>52K</td>\n",
       "      <td>2048</td>\n",
       "      <td>-</td>\n",
       "      <td>158GB</td>\n",
       "      <td>850B</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-05</td>\n",
       "      <td>StarCoder</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>15.5B</td>\n",
       "      <td>49K</td>\n",
       "      <td>8192</td>\n",
       "      <td>-</td>\n",
       "      <td>815GB</td>\n",
       "      <td>1T</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-06</td>\n",
       "      <td>Phi-1</td>\n",
       "      <td>GPT-J</td>\n",
       "      <td>1.3B</td>\n",
       "      <td>51K</td>\n",
       "      <td>2048</td>\n",
       "      <td>-</td>\n",
       "      <td>7B</td>\n",
       "      <td>53B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-10</td>\n",
       "      <td>CodeFuse</td>\n",
       "      <td>GPT-J</td>\n",
       "      <td>350M-13B</td>\n",
       "      <td>101K</td>\n",
       "      <td>4096</td>\n",
       "      <td>-</td>\n",
       "      <td>1.6TB/1T</td>\n",
       "      <td>Ant Group</td>\n",
       "      <td>40+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-10</td>\n",
       "      <td>CodeShell</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>7B</td>\n",
       "      <td>70K</td>\n",
       "      <td>8192</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>500B</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-10</td>\n",
       "      <td>PyMT5</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>374M</td>\n",
       "      <td>50K</td>\n",
       "      <td>1024+1024</td>\n",
       "      <td>-</td>\n",
       "      <td>27GB</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-02</td>\n",
       "      <td>Mastropaolo et al.</td>\n",
       "      <td>T5</td>\n",
       "      <td>60M</td>\n",
       "      <td>32k</td>\n",
       "      <td>512+512</td>\n",
       "      <td>-</td>\n",
       "      <td>1GB</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-02</td>\n",
       "      <td>DOBF</td>\n",
       "      <td>-</td>\n",
       "      <td>250M</td>\n",
       "      <td>50K</td>\n",
       "      <td>512+512</td>\n",
       "      <td>-</td>\n",
       "      <td>45GB</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-03</td>\n",
       "      <td>PLBART</td>\n",
       "      <td>BART</td>\n",
       "      <td>140M</td>\n",
       "      <td>50K</td>\n",
       "      <td>1024+1024</td>\n",
       "      <td>-</td>\n",
       "      <td>655GB/71B</td>\n",
       "      <td>210B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-09</td>\n",
       "      <td>CodeT5</td>\n",
       "      <td>T5</td>\n",
       "      <td>60M-220M</td>\n",
       "      <td>32K</td>\n",
       "      <td>512+256</td>\n",
       "      <td>-</td>\n",
       "      <td>∼25GB</td>\n",
       "      <td>-</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-01</td>\n",
       "      <td>SPT-Code</td>\n",
       "      <td>BART</td>\n",
       "      <td>262M</td>\n",
       "      <td>80K</td>\n",
       "      <td>512+512</td>\n",
       "      <td>-</td>\n",
       "      <td>20GB</td>\n",
       "      <td>-</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022-02</td>\n",
       "      <td>AlphaCode</td>\n",
       "      <td>-</td>\n",
       "      <td>300M-41B</td>\n",
       "      <td>8K</td>\n",
       "      <td>1536+768</td>\n",
       "      <td>-</td>\n",
       "      <td>715GB</td>\n",
       "      <td>967B</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-06</td>\n",
       "      <td>NatGen</td>\n",
       "      <td>T5</td>\n",
       "      <td>220M</td>\n",
       "      <td>32K</td>\n",
       "      <td>512+256</td>\n",
       "      <td>CodeT5</td>\n",
       "      <td>∼26GB</td>\n",
       "      <td>14B</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-05</td>\n",
       "      <td>CodeT5+</td>\n",
       "      <td>T5/GPT-3</td>\n",
       "      <td>220M-16B</td>\n",
       "      <td>50K</td>\n",
       "      <td>2048+2048</td>\n",
       "      <td>CodeGen-mon</td>\n",
       "      <td>52B</td>\n",
       "      <td>-</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-12</td>\n",
       "      <td>CugLM</td>\n",
       "      <td>BERT</td>\n",
       "      <td>51M</td>\n",
       "      <td>50K</td>\n",
       "      <td>128</td>\n",
       "      <td>-</td>\n",
       "      <td>8M</td>\n",
       "      <td>1.2B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022-03</td>\n",
       "      <td>UniXcoder</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>125M</td>\n",
       "      <td>51K</td>\n",
       "      <td>1024</td>\n",
       "      <td>-</td>\n",
       "      <td>20GB+</td>\n",
       "      <td>839B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date                Model     Arch.        Size Vocab    Context  \\\n",
       "0   2019-12               CuBERT      BERT        350M   50K       1024   \n",
       "1   2020-02             CodeBERT   RoBERTa        125M   50K        512   \n",
       "2   2020-09       GraphCode-BERT   RoBERTa        125M   50K        640   \n",
       "3   2021-08            SynCoBERT   RoBERTa        125M   50K        512   \n",
       "4   2021-10                DISCO      BERT        100M   20K        512   \n",
       "5   2022-05             Code-MVP   RoBERTa        125M   50K        512   \n",
       "6   2020-05                GPT-C     GPT-2        374M   60K       1024   \n",
       "7   2021-02              CodeGPT     GPT-2        124M   50K       1024   \n",
       "8   2022-02            PolyCoder     GPT-2   160M-2.7B   50K       2048   \n",
       "9   2022-03  CodeGen-Multi(Mono)     GPT-3  350M-16.1B   50K       2048   \n",
       "10  2022-04              InCoder     GPT-3        6.7B   50K       2048   \n",
       "11  2022-06            PyCodeGPT   GPT-Neo        110M   32K       1024   \n",
       "12  2022-07          PanGu-Coder   PanGu-α   317M-2.6B   42K       1024   \n",
       "13  2023-01           SantaCoder     GPT-2        1.1B   49K       2048   \n",
       "14  2023-03             CodeGeeX   PanGu-α         13B   52K       2048   \n",
       "15  2023-05            StarCoder     GPT-2       15.5B   49K       8192   \n",
       "16  2023-06                Phi-1     GPT-J        1.3B   51K       2048   \n",
       "17  2023-10             CodeFuse     GPT-J    350M-13B  101K       4096   \n",
       "18  2023-10            CodeShell     GPT-2          7B   70K       8192   \n",
       "19  2020-10                PyMT5     GPT-2        374M   50K  1024+1024   \n",
       "20  2021-02   Mastropaolo et al.        T5         60M   32k    512+512   \n",
       "21  2021-02                 DOBF         -        250M   50K    512+512   \n",
       "22  2021-03               PLBART      BART        140M   50K  1024+1024   \n",
       "23  2021-09               CodeT5        T5    60M-220M   32K    512+256   \n",
       "24  2022-01             SPT-Code      BART        262M   80K    512+512   \n",
       "25  2022-02            AlphaCode         -    300M-41B    8K   1536+768   \n",
       "26  2022-06               NatGen        T5        220M   32K    512+256   \n",
       "27  2023-05              CodeT5+  T5/GPT-3    220M-16B   50K  2048+2048   \n",
       "28  2020-12                CugLM      BERT         51M   50K        128   \n",
       "29  2022-03            UniXcoder   RoBERTa        125M   51K       1024   \n",
       "\n",
       "       Init. from                  Dataset   Training    PL  \n",
       "0               -                     9.3B        93B     1  \n",
       "1         RoBERTa                     20GB       105B     6  \n",
       "2        CodeBERT                     20GB       131B     6  \n",
       "3        CodeBERT                     20GB         7B     6  \n",
       "4               -                    1.8GB          -     2  \n",
       "5   GraphCodeBERT                      2GB        39B     1  \n",
       "6               -                      11B       270B     4  \n",
       "7           GPT-2                      2GB          -     1  \n",
       "8               -                    254GB        39B    12  \n",
       "9               -  1.6TB(1.8TB)/506B(577B)   1T(1.2T)  6(1)  \n",
       "10              -                    204GB        52B    28  \n",
       "11              -                     96GB       100B     1  \n",
       "12              -                    147GB       230B     1  \n",
       "13              -                    268GB       236B     3  \n",
       "14              -                    158GB       850B    23  \n",
       "15              -                    815GB         1T    86  \n",
       "16              -                       7B        53B     1  \n",
       "17              -                 1.6TB/1T  Ant Group   40+  \n",
       "18              -                        -       500B     -  \n",
       "19              -                     27GB          -     1  \n",
       "20              -                      1GB          -     1  \n",
       "21              -                     45GB          -     2  \n",
       "22              -                655GB/71B       210B     2  \n",
       "23              -                    ∼25GB          -     8  \n",
       "24              -                     20GB          -     6  \n",
       "25              -                    715GB       967B    13  \n",
       "26         CodeT5                    ∼26GB        14B     8  \n",
       "27    CodeGen-mon                      52B          -     9  \n",
       "28              -                       8M       1.2B     2  \n",
       "29              -                    20GB+       839B     6  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "datasets = [\n",
    "    [\"Dataset\", \"Language\", \"Size\", \"Description\"],\n",
    "    [\"CodeXGLUE\", \"Python\", \"10k samples\", \"Benchmark dataset for code intelligence\"],\n",
    "    [\"HumanEval\", \"Python\", \"77k samples\", \"Diverse code samples for training foundations models\"], \n",
    "    [\"CodeParrot\", \"Python\", \"35 million samples\", \"Large dataset of Python functions for few-shot learning\"],\n",
    "    [\"CodeForce\", \"C++\", \"435k problems\", \"Competitive programming challenges\"],\n",
    "    [\"GitHub\", \"Various\", \"Billions of lines\", \"Open source code from public GitHub repositories\"],\n",
    "]\n",
    "\n",
    "with open('code_datasets.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_start_X = 10**6\n",
    "range_end_X = 10**10\n",
    "range_step_X = 5*10**6\n",
    "\n",
    "x_vals = list(range(range_start_X, range_end_X, range_step_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
