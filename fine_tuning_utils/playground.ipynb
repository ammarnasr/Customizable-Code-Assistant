{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer_og = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B\")\n",
    "model_og = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen2-1B\", trust_remote_code=True, revision=\"main\")\n",
    "model_og = model_og.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(\"ammarnasr/CodeGen2_1B_merged\")\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(\"ammarnasr/CodeGen2_1B_merged\", trust_remote_code=True, revision=\"main\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_problems = [\n",
    "    \"def check_password_strength(password: str) -> bool:\\n    '''Check the strength of a password based on certain criteria (e.g., length, complexity).'''\\n\",\n",
    "    \"def sanitize_input(input_str: str) -> str:\\n    '''Implement a function to sanitize user input to prevent XSS (Cross-Site Scripting) attacks.'''\\n\",\n",
    "    \"def prevent_sql_injection(query: str) -> str:\\n    '''Implement a function to sanitize SQL queries and prevent SQL injection attacks.'''\\n\"\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "  text = security_problems[i]\n",
    "  input_ids_og = tokenizer_og(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "  input_ids_ft = tokenizer_ft(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "  generated_ids_og = model_og.generate(input_ids_og, max_length=128)\n",
    "  generated_ids_ft = model_ft.generate(input_ids_ft, max_length=128)\n",
    "\n",
    "  out_og = tokenizer_og.decode(generated_ids_og[0], skip_special_tokens=True)\n",
    "  out_ft = tokenizer_ft.decode(generated_ids_ft[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "  print('-'*80)\n",
    "  print('OG:')\n",
    "  print(out_og)\n",
    "  print()\n",
    "  print('FT:')\n",
    "  print(out_ft)\n",
    "  print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_new_tokens\": 60,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] ==8 else torch.float16\n",
    "\n",
    "class EndpointHandler:\n",
    "    def __init__(self, path=\"\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=True, revision=\"main\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def __call__(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        prompt = data[\"inputs\"]\n",
    "        if \"config\" in data:\n",
    "          config = data.pop(\"config\", None)\n",
    "        else:\n",
    "          config = {'max_new_tokens':100}\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        generated_ids = self.model.generate(input_ids, **config)\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return [{\"generated_text\": generated_text}]\n",
    "        \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def prevent_sql_injection(query: str) -> str:\n",
      "    '''Implement a function to sanitize SQL queries and prevent SQL injection attacks.'''\n",
      "(),\n",
      "ï¿½/orequesthasK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/ammarnasr/CodeGen2_1B_merged\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "prompt = \"\"\"\n",
    "def prevent_sql_injection(query: str) -> str:\n",
    "    '''Implement a function to sanitize SQL queries and prevent SQL injection attacks.'''\n",
    "\"\"\"\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": prompt\n",
    "})\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://q02qcgkwagbft8mt.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Authorization\": \"\",\n",
    "\t\"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "prompt = \"\"\"\n",
    "def prevent_sql_injection(query: str) -> str:\n",
    "    '''Prevent SQL injection attacks.'''\n",
    "\"\"\"\n",
    "output = query({\n",
    "\t\"inputs\":prompt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def prevent_sql_injection(query: str) -> str:\n",
      "    '''Prevent SQL injection attacks.'''\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datasets  import  load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Downloading and loading a dataset from the hub.\n",
    "raw_datasets = load_dataset(\"ammarnasr/Customizable-Code-Assistant-Data\")\n",
    "\n",
    "# Filtering the dataset to only Python examples.\n",
    "raw_datasets = raw_datasets.filter(lambda example: example['language'] == 'Python')\n",
    "\n",
    "# Splitting the dataset into train, test, and validation sets.\n",
    "train_testvalid = raw_datasets['train'].train_test_split(0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(0.5)\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "context_length = 128\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "print(f\"Tokenized dataset: {tokenized_datasets['train'].num_rows} training samples\")\n",
    "      \n",
    "# Preparing the model\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# Preparing the data collator\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Preparing the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"gpt2-from-scratch-customizable-code-assistant\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Math : \n",
    "# basic math related to computation and memory usage for transformers\n",
    "\n",
    "# 1. Information About Local Accelerator (GPU/CPU)\n",
    "# This includes all the available devices and each one's :\n",
    "# Name, Compute Capability, Multiprocessors, CUDA Cores, Concurrent threads, GPU clock, Memory clock, Total Memory and Free Memory.\n",
    "from cuda_utils import  main, gpu_flops\n",
    "import json\n",
    "cuda_info = main()\n",
    "print(json.dumps(cuda_info,  indent=4))\n",
    "\n",
    "# 2. Compute Requirements\n",
    "# The basic equation giving the cost to train a transformer model is given by:\n",
    "#                                    C = tao*T = 6*P*D\n",
    "# where:\n",
    "# C is the compute required to train the transformer model, in total floating point operations (FLOPs)\n",
    "# C = C_forward + C_backward\n",
    "# C_forward  =  2*P*D\n",
    "# C_backward =  4*P*D\n",
    "# tao is the aggregate throughput of your hardware setup (tao = (No. of GPUs) * (Actual FLOPs/GPU), in FLOPs\n",
    "# T is the time spent training the model, in seconds\n",
    "# P is the number of parameters in the transformer model\n",
    "# D is the dataset size, in tokens\n",
    "\n",
    "#3. Estimating GPU Actual FLOPs (tao)\n",
    "# Estimating GPU FLOPs accurately can be challenging due to the complexity of GPU architectures and optimizations. However, here is a simple formula to estimate a GPU FLOP/s:\n",
    "# Total GPU FLOPS/s = GPU clock * cores * flops_per_clock_cycle * fp_precision\n",
    "for gpu_no in cuda_info:\n",
    "    gpu = cuda_info[gpu_no]\n",
    "    print(f\"GPU: {gpu['Name']}\")\n",
    "    print(f\"GPU FLOPS: {gpu_flops(gpu):.2f} TFLOPS\")\n",
    "    tao = gpu_flops(gpu) # TFLOPS\n",
    "\n",
    "#4. Estimating Model Parameters (P)\n",
    "P = model.num_parameters()\n",
    "print(f\"Model Parameters: {P/1000**2:.2f}M\")\n",
    "\n",
    "#5. Estimating Dataset Size (D)\n",
    "D = 0\n",
    "for input_ids in tokenized_datasets[\"train\"][\"input_ids\"]:\n",
    "    D += len(input_ids)\n",
    "print(f\"Dataset Size: {D} tokens\")\n",
    "\n",
    "#6. Estimating Training Time (T)\n",
    "# Calculate training time based on the formula tao*T = 6*P*D. Note that ao needs to be in FLOPS/s not TFLOPS/s, to convert TFLOPS/s to FLOPS/s multiply by 1000**3.\n",
    "T = 6*P*D/(tao*1000**3)\n",
    "print(f\"Training Time: {T/3600:.2f} hours\")\n",
    "\n",
    "\n",
    "#7. Parameter vs Dataset Tradeoffs\n",
    "# Although strictly speaking you can train a transformer for as many tokens as you like, the number of tokens trained can highly impact both the computing costs and the final model performance making striking the right balance important.\n",
    "# compute optimal language model has a number of parameters and a dataset size that satisfies the approximation D = 20*P.\n",
    "optimal_D = 20*P\n",
    "print(f\"Optimal Dataset Size: {optimal_D / 1000**2:.2f}M tokens ({optimal_D / D:.2f}x current dataset size)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda_utils import  main, list_common_gpus, custome_gpu_info\n",
    "import json\n",
    "\n",
    "available_gpus = list_common_gpus()\n",
    "print(f\"Available GPUs: {available_gpus}\")\n",
    "\n",
    "\n",
    "t4_cuda_info = custome_gpu_info('NVIDIA T4')\n",
    "cuda_info = main(verbose=False)\n",
    "print(json.dumps(t4_cuda_info,  indent=4))\n",
    "print('------------------')\n",
    "print(json.dumps(cuda_info,  indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('llms_info.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "datasets = [\n",
    "    [\"Dataset\", \"Language\", \"Size\", \"Description\"],\n",
    "    [\"CodeXGLUE\", \"Python\", \"10k samples\", \"Benchmark dataset for code intelligence\"],\n",
    "    [\"HumanEval\", \"Python\", \"77k samples\", \"Diverse code samples for training foundations models\"], \n",
    "    [\"CodeParrot\", \"Python\", \"35 million samples\", \"Large dataset of Python functions for few-shot learning\"],\n",
    "    [\"CodeForce\", \"C++\", \"435k problems\", \"Competitive programming challenges\"],\n",
    "    [\"GitHub\", \"Various\", \"Billions of lines\", \"Open source code from public GitHub repositories\"],\n",
    "]\n",
    "\n",
    "with open('code_datasets.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_start_X = 10**6\n",
    "range_end_X = 10**10\n",
    "range_step_X = 5*10**6\n",
    "\n",
    "x_vals = list(range(range_start_X, range_end_X, range_step_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
