{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib \n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "def calc_avg_line_length(text):\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    return sum(line_lengths) / len(line_lengths)\n",
    "\n",
    "def calc_max_line_length(text):\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    return max(line_lengths)\n",
    "\n",
    "def calc_alphanum_fraction(text):\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    alphanum = sum(c.isalnum() for c in text)\n",
    "    return alphanum / len(text)\n",
    "\n",
    "def dataset_from_df(df):\n",
    "    dataset = {\n",
    "        'repo_name': [],\n",
    "        'repo_url': [],\n",
    "        'repo_description': [],\n",
    "        'repo_stars': [],\n",
    "        'repo_forks': [],\n",
    "        'repo_last_updated': [],\n",
    "        'repo_created_at': [],\n",
    "        'repo_size': [],\n",
    "        'repo_license': [],\n",
    "        'language': [],\n",
    "        'text': [],\n",
    "        'avg_line_length': [],\n",
    "        'max_line_length': [],\n",
    "        'alphnanum_fraction': [],\n",
    "    }\n",
    "    for i in tqdm(range(len(df))):\n",
    "        repo = df.iloc[i]\n",
    "        code = repo['code']\n",
    "        for programming_language in code:\n",
    "            code_files = code[programming_language]\n",
    "            for code_file in code_files:\n",
    "                text = code_files[code_file]\n",
    "                dataset['repo_name'].append(repo['name'])\n",
    "                dataset['repo_url'].append(repo['url'])\n",
    "                dataset['repo_description'].append(repo['description'])\n",
    "                dataset['repo_stars'].append(repo['stars'])\n",
    "                dataset['repo_forks'].append(repo['forks'])\n",
    "                dataset['repo_last_updated'].append(repo['last_updated'])\n",
    "                dataset['repo_created_at'].append(repo['created'])\n",
    "                dataset['repo_size'].append(repo['size'])\n",
    "                dataset['repo_license'].append(repo['license'])\n",
    "                dataset['language'].append(programming_language)\n",
    "                dataset['text'].append(text)\n",
    "                dataset['avg_line_length'].append(calc_avg_line_length(text))\n",
    "                dataset['max_line_length'].append(calc_max_line_length(text))\n",
    "                dataset['alphnanum_fraction'].append(calc_alphanum_fraction(text))\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    return dataset\n",
    "\n",
    "def huggingface_dataset_from_df(df):\n",
    "    dataset = dataset_from_df(df)\n",
    "    with open('hf_ds.pkl', 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    hf_dataset = load_dataset(\"pandas\", data_files='hf_ds.pkl')\n",
    "    os.remove('hf_ds.pkl')\n",
    "    return hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblibs_path = '.././saved_searches'\n",
    "joblibs = os.listdir(joblibs_path)\n",
    "joblibs = [f for f in joblibs if f.endswith('.joblib')]\n",
    "joblibs = [os.path.join(joblibs_path, f) for f in joblibs]\n",
    "df = joblib.load(joblibs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = huggingface_dataset_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.filter(lambda example: example['language'] == 'Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 90% train, 10% test + validation\n",
    "train_testvalid = raw_datasets['train'].train_test_split(0.1)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"gpt2-from-scratch-customizable-code-assistant\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
